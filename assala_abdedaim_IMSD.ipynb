{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5916e791-bd3d-41d8-916d-86506c309c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.263327211206297\n",
      "Epoch 100, Loss: 0.2496567741445596\n",
      "Epoch 200, Loss: 0.24955228505461724\n",
      "Epoch 300, Loss: 0.24943098085528878\n",
      "Epoch 400, Loss: 0.2492851914829899\n",
      "Epoch 500, Loss: 0.2491046665168712\n",
      "Epoch 600, Loss: 0.2488751959696997\n",
      "Epoch 700, Loss: 0.24857656177916976\n",
      "Epoch 800, Loss: 0.24817943349167876\n",
      "Epoch 900, Loss: 0.24764058730481286\n",
      "Epoch 1000, Loss: 0.246895389052751\n",
      "Epoch 1100, Loss: 0.24584558225684133\n",
      "Epoch 1200, Loss: 0.2443385110262897\n",
      "Epoch 1300, Loss: 0.24214431614475068\n",
      "Epoch 1400, Loss: 0.2389014779629115\n",
      "Epoch 1500, Loss: 0.23402975370357335\n",
      "Epoch 1600, Loss: 0.22666371088333848\n",
      "Epoch 1700, Loss: 0.21574924200495282\n",
      "Epoch 1800, Loss: 0.199947589419715\n",
      "Epoch 1900, Loss: 0.1758876317654965\n",
      "Epoch 2000, Loss: 0.13355890592074504\n",
      "Epoch 2100, Loss: 0.07258987450282711\n",
      "Epoch 2200, Loss: 0.03222877167510081\n",
      "Epoch 2300, Loss: 0.017241782254947203\n",
      "Epoch 2400, Loss: 0.011494864682969515\n",
      "Epoch 2500, Loss: 0.008171515142374278\n",
      "Epoch 2600, Loss: 0.006272704932409363\n",
      "Epoch 2700, Loss: 0.005091174706591692\n",
      "Epoch 2800, Loss: 0.004261284226599443\n",
      "Epoch 2900, Loss: 0.0035442178603102142\n",
      "Epoch 3000, Loss: 0.0030836463713776628\n",
      "Epoch 3100, Loss: 0.0027235670835782204\n",
      "Epoch 3200, Loss: 0.0024261534777288746\n",
      "Epoch 3300, Loss: 0.002193937234990292\n",
      "Epoch 3400, Loss: 0.001977318466956035\n",
      "Epoch 3500, Loss: 0.0018110607444854758\n",
      "Epoch 3600, Loss: 0.0016687527502907116\n",
      "Epoch 3700, Loss: 0.0015427222330813622\n",
      "Epoch 3800, Loss: 0.001434746550066544\n",
      "Epoch 3900, Loss: 0.00133881056550624\n",
      "Epoch 4000, Loss: 0.0012563519778626398\n",
      "Epoch 4100, Loss: 0.001179217055887036\n",
      "Epoch 4200, Loss: 0.00111874025732492\n",
      "Epoch 4300, Loss: 0.0010543907989586164\n",
      "Epoch 4400, Loss: 0.0010001614439648708\n",
      "Epoch 4500, Loss: 0.0009515319771787021\n",
      "Epoch 4600, Loss: 0.000910916187636063\n",
      "Epoch 4700, Loss: 0.0008633828566086991\n",
      "Epoch 4800, Loss: 0.0008262303807886767\n",
      "Epoch 4900, Loss: 0.0007915055954231065\n",
      "Epoch 5000, Loss: 0.0007601329043016818\n",
      "Epoch 5100, Loss: 0.0007296383836283222\n",
      "Epoch 5200, Loss: 0.0007018618187496755\n",
      "Epoch 5300, Loss: 0.00067706134822155\n",
      "Epoch 5400, Loss: 0.0006530997090170491\n",
      "Epoch 5500, Loss: 0.0006332593281615911\n",
      "Epoch 5600, Loss: 0.0006088450820225515\n",
      "Epoch 5700, Loss: 0.0005899083161188923\n",
      "Epoch 5800, Loss: 0.000571564638549023\n",
      "Epoch 5900, Loss: 0.0005545277223779696\n",
      "Epoch 6000, Loss: 0.0005383511024280825\n",
      "Epoch 6100, Loss: 0.000521817421466977\n",
      "Epoch 6200, Loss: 0.0005071843772795075\n",
      "Epoch 6300, Loss: 0.0004932953836015972\n",
      "Epoch 6400, Loss: 0.00047976708944028133\n",
      "Epoch 6500, Loss: 0.0004670471084355349\n",
      "Epoch 6600, Loss: 0.00045538271676556574\n",
      "Epoch 6700, Loss: 0.00044436211147686844\n",
      "Epoch 6800, Loss: 0.00043272628315201304\n",
      "Epoch 6900, Loss: 0.00042331832468059773\n",
      "Epoch 7000, Loss: 0.0004129529556348227\n",
      "Epoch 7100, Loss: 0.0004037527304387138\n",
      "Epoch 7200, Loss: 0.000394789405826962\n",
      "Epoch 7300, Loss: 0.00038619115291863135\n",
      "Epoch 7400, Loss: 0.0003776643727335699\n",
      "Epoch 7500, Loss: 0.00037033577507780013\n",
      "Epoch 7600, Loss: 0.0003614622766212785\n",
      "Epoch 7700, Loss: 0.00035452563551017374\n",
      "Epoch 7800, Loss: 0.0003470828910362196\n",
      "Epoch 7900, Loss: 0.00034033624014512384\n",
      "Epoch 8000, Loss: 0.0003338397662698741\n",
      "Epoch 8100, Loss: 0.0003273234753480475\n",
      "Epoch 8200, Loss: 0.0003213672767873964\n",
      "Epoch 8300, Loss: 0.00031585090388423006\n",
      "Epoch 8400, Loss: 0.0003102154352164967\n",
      "Epoch 8500, Loss: 0.0003041359232155151\n",
      "Epoch 8600, Loss: 0.00029892066487234613\n",
      "Epoch 8700, Loss: 0.0002941156353607129\n",
      "Epoch 8800, Loss: 0.00028869818525511213\n",
      "Epoch 8900, Loss: 0.0002840447517327029\n",
      "Epoch 9000, Loss: 0.00027923015424005285\n",
      "Epoch 9100, Loss: 0.0002750328959652934\n",
      "Epoch 9200, Loss: 0.00027044713073735925\n",
      "Epoch 9300, Loss: 0.0002661044018181534\n",
      "Epoch 9400, Loss: 0.000262374648331444\n",
      "Epoch 9500, Loss: 0.00025811071992796737\n",
      "Epoch 9600, Loss: 0.00025437692183818305\n",
      "Epoch 9700, Loss: 0.00025055206416862714\n",
      "Epoch 9800, Loss: 0.0002466810939991501\n",
      "Epoch 9900, Loss: 0.0002431387372183103\n",
      "\n",
      "Prédictions après entraînement :\n",
      "Entrée: [0 0], Prédiction: 0.0196, Attendu: 0\n",
      "Entrée: [0 1], Prédiction: 0.9904, Attendu: 1\n",
      "Entrée: [1 0], Prédiction: 0.9902, Attendu: 1\n",
      "Entrée: [1 1], Prédiction: 0.0196, Attendu: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonctions d'activation\n",
    "def sigmoid(x):\n",
    "    result = 1 / (1 + np.exp(-np.clip(x, -500, 500))) # Clipping pour stabilité\n",
    "    assert np.all((result >= 0) & (result <= 1)), \"Sigmoïde doit être entre 0 et 1\"\n",
    "    return result\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    result = s * (1 - s)\n",
    "    assert np.all(result >= 0), \"Dérivée de la sigmoïde doit être >= 0\"\n",
    "    if np.isscalar(x) and x == 0:\n",
    "        assert np.isclose(result, 0.25), \"Dérivée de la sigmoïde à 0 doit être 0.25\"\n",
    "    return result\n",
    "\n",
    "def relu(x):\n",
    "    result = np.maximum(0, x)\n",
    "    assert np.all(result >= 0), \"ReLU doit être >= 0\"\n",
    "    return result\n",
    "\n",
    "def relu_derivative(x):\n",
    "    result = np.where(x > 0, 1, 0)\n",
    "    assert np.all((result == 0) | (result == 1)), \"Dérivée de ReLU doit être 0 ou 1\"\n",
    "    return result\n",
    "\n",
    "# Classe Layer\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # Initialisation adaptée à l'activation\n",
    "        if activation == 'sigmoid':\n",
    "            # Initialisation Xavier\n",
    "            self.W = np.random.randn(input_size, output_size) * np.sqrt(1. / input_size)\n",
    "        elif activation == 'relu':\n",
    "            # Initialisation He\n",
    "            self.W = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n",
    "        else:\n",
    "            raise ValueError(\"Activation non supportée\")\n",
    "        self.b = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "        # Assertions pour vérifier les dimensions\n",
    "        assert self.W.shape == (input_size, output_size), \"Mauvaise dimension pour W\"\n",
    "        assert self.b.shape == (1, output_size), \"Mauvaise dimension pour b\"\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.b\n",
    "        if self.activation == 'sigmoid':\n",
    "            self.A = sigmoid(self.Z)\n",
    "        elif self.activation == 'relu':\n",
    "            self.A = relu(self.Z)\n",
    "        else:\n",
    "            raise ValueError(\"Activation non supportée\")\n",
    "        assert self.A.shape == (X.shape[0], self.W.shape[1]), \"Mauvaise dimension pour A\"\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA, learning_rate):\n",
    "        if self.activation == 'sigmoid':\n",
    "            dZ = dA * sigmoid_derivative(self.Z)\n",
    "        elif self.activation == 'relu':\n",
    "            dZ = dA * relu_derivative(self.Z)\n",
    "        else:\n",
    "            raise ValueError(\"Activation non supportée\")\n",
    "        \n",
    "        # Clipping des gradients pour stabilité\n",
    "        dZ = np.clip(dZ, -100, 100)\n",
    "        \n",
    "        m = self.X.shape[0]\n",
    "        dW = (1 / m) * np.dot(self.X.T, dZ)\n",
    "        db = (1 / m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        \n",
    "        # Assertions pour vérifier les dimensions\n",
    "        assert dW.shape == self.W.shape, \"Mauvaise dimension pour dW\"\n",
    "        assert db.shape == self.b.shape, \"Mauvaise dimension pour db\"\n",
    "        \n",
    "        # Mise à jour des paramètres\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        # Gradient pour la couche précédente\n",
    "        dA_prev = np.dot(dZ, self.W.T)\n",
    "        return dA_prev\n",
    "\n",
    "# Classe Model\n",
    "class Model:\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        self.layers = layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        loss = np.mean((y_true - y_pred) ** 2)\n",
    "        assert loss >= 0, \"La perte doit être positive ou nulle\"\n",
    "        if np.array_equal(y_true, y_pred):\n",
    "            assert np.isclose(loss, 0), \"La perte doit être 0 si y_true = y_pred\"\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, y_pred):\n",
    "        m = X.shape[0]\n",
    "        dA = (y_pred - y) # Gradient initial pour la couche de sortie\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA, self.lr)\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "            self.backward(X, y, y_pred)\n",
    "\n",
    "    def test(self, X, y):\n",
    "        predictions = self.forward(X)\n",
    "        print(\"\\nPrédictions après entraînement :\")\n",
    "        for i in range(len(X)):\n",
    "            print(f\"Entrée: {X[i]}, Prédiction: {predictions[i][0]:.4f}, Attendu: {y[i][0]}\")\n",
    "            if y[i][0] == 0:\n",
    "                assert predictions[i][0] < 0.5, f\"Prédiction pour {X[i]} devrait être < 0.5\"\n",
    "            else:\n",
    "                assert predictions[i][0] > 0.5, f\"Prédiction pour {X[i]} devrait être > 0.5\"\n",
    "\n",
    "# Test du modèle\n",
    "if __name__ == \"__main__\":\n",
    "    # Données XOR\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    # Création des couches\n",
    "    layers = [\n",
    "        Layer(input_size=2, output_size=4, activation='sigmoid', seed=42),\n",
    "        Layer(input_size=4, output_size=3, activation='relu', seed=42),\n",
    "        Layer(input_size=3, output_size=1, activation='sigmoid', seed=42)\n",
    "    ]\n",
    "\n",
    "    # Création et entraînement du modèle\n",
    "    model = Model(layers, learning_rate=0.5)\n",
    "    model.train(X, y, epochs=10000)\n",
    "    model.test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228e645-987a-495c-aad9-dcd9ca8bf633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
